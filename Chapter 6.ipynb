{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Chapter 6.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6oUZeKEZgEH",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZCfS1CgabyC",
        "colab_type": "text"
      },
      "source": [
        "## Problem\n",
        "\n",
        "Information retrieval using word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kxrjCHgk-6r",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-1 Import the libraries\n",
        "Here are the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRW6zxGsZgEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c2ec70ed-e213-4cb1-8369-a35be00e673c"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np \n",
        "import nltk\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import scipy \n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')  \n",
        "print(stopword_list) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTno6UalI_r",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-2 Create/import documents\n",
        "Randomly taking sentences from the internet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phlhq5yKZgEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "593695b3-9a6c-4a4a-9b97-b58a09c23c3b"
      },
      "source": [
        "# Randomly taking sentences from internet \n",
        "\n",
        "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ] \n",
        "     \n",
        "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
        "\n",
        "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\n",
        "\n",
        "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]\n",
        "\n",
        "# Put all the documents in one list\n",
        "\n",
        "fin= Doc1+Doc2+Doc3+Doc4\n",
        "\n",
        "print(fin)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.', 'Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.', 'He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.', 'But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtXNOWr_lVTG",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-3 Download word2vec\n",
        "As mentioned earlier, we are going to use the word embeddings to solve\n",
        "this problem. Download word2vec from the below link:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWclK0vIle9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "85935873-90de-4f92-c536-5203592f36fc"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#!wget -P /root/input/ -c \"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "#load the model\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary = True);\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-25 22:32:19--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.93.13\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.93.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHd1Vb-wO1N",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-4 Create IR [information retrieval] system\n",
        "Now we build the information retrieval system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCPVRIf3ZgEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing \n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' \n",
        "    text = re.sub(pattern, '', ''.join(text))\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
        "\n",
        "def get_embedding(word):\n",
        "    if word in model.wv.vocab:\n",
        "        #return model[x]\n",
        "        return model[word]\n",
        "    else:\n",
        "        return np.zeros(300)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOxWM3Iwq_r",
        "colab_type": "text"
      },
      "source": [
        "For every document, we will get a lot of vectors based on the number of\n",
        "words present. We need to calculate the average vector for the document\n",
        "through taking a mean of all the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY1g7SglZgER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3c0763a9-600b-41d1-fb78-5d87bd40f96b"
      },
      "source": [
        "#nltk.download('punkt')\n",
        "\n",
        "# Getting average vector for each document \n",
        "out_dict =  {}\n",
        "for sen in fin:\n",
        "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
        "    dict = { sen : (average_vector) }\n",
        "    out_dict.update(dict)\n",
        "\n",
        "# Function to calculate the similarity between the query vector and document vector\n",
        "\n",
        "def get_sim(query_embedding, average_vector_doc):\n",
        "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
        "    return sim\n",
        "\n",
        "# Rank all the documents based on the similarity to get relevant docs\n",
        "\n",
        "def Ranked_documents(query):\n",
        "    query_words =  (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
        "    rank = []\n",
        "    for k,v in out_dict.items():\n",
        "        rank.append((k, get_sim(query_words, v)))\n",
        "    rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
        "    print('Ranked Documents :')\n",
        "    return rank\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Ck5wxFxQsS",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-5 Results and applications\n",
        "Let’s see how the information retrieval system we built is working with a\n",
        "couple of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjMsJCqpZgEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4b8b2563-8950-47c4-e2f6-9dc833e32115"
      },
      "source": [
        "# Call the IR function with a query\n",
        "# If you see, doc4 (on top in result), this will be most relevant for the\n",
        "# query “cricket” even though the word “cricket” is not even mentioned once\n",
        "# with the similarity of 0.449.\n",
        "\n",
        "Ranked_documents(\"cricket\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranked Documents :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.44954328830341783]),\n",
              " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.23973446930269127]),\n",
              " ('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.1832371201201335]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.17995061678671642])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szKnBM3CZgEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d46ceebf-cd27-43b3-a8fd-48b39d96328c"
      },
      "source": [
        "#Let’s take one more example as may be driving.\n",
        "# Again, since driving is connected to transport and the Motor Vehicles\n",
        "# Act, it pulls out the most relevant documents on top. The first 2 documents\n",
        "# are relevant to the query. \n",
        "\n",
        "Ranked_documents(\"driving\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranked Documents :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.3594728772380067]),\n",
              " ('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.19042557661139026]),\n",
              " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.1706653724240128]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.08872308406410134])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqvGS5xa6oQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download spam.csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "csv_url = \"https://github.com/alberwan/Test-Data/blob/master/spam.csv\"\n",
        "\n",
        "# csv_url = \"https://www.kaggle.com/ishansoni/sms-spam-collection-dataset?select=spam.csv\"\n",
        "s = requests.get(csv_url).content\n",
        "file_content = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "# file_content = pd.read_csv(csv_url)\n",
        "file_content.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWxAmCbZgEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read file\n",
        "# file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
        "file_content = pd.read_csv(\"https://github.com/alberwan/Test-Data/blob/master/spam.csv\", header=None)\n",
        "\n",
        "#check sample content in the email\n",
        "file_content['v2'][1]\n",
        "\n",
        "#Import library\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Remove stop words\n",
        "stop = stopwords.words('english')\n",
        "file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "# Delete unwanted columns\n",
        "Email_Data = file_content[['v1', 'v2']]\n",
        "\n",
        "# Rename column names\n",
        "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
        "Email_Data.head()\n",
        "\n",
        "#Delete punctuations, convert text in lower case and delete the double space \n",
        "\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub(' ', ' ', x))\n",
        "Email_Data['Email'].head(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YObrUqR6ZgEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separating text(input) and target classes\n",
        "\n",
        "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
        "list_classes = [\"Target\"]\n",
        "target = Email_Data[list_classes].values\n",
        "\n",
        "\n",
        "To_Process=Email_Data[['Email', 'Target']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiLUaKc1ZgEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and test split with 80:20 ratio\n",
        "train, test = train_test_split(To_Process, test_size=0.2) \n",
        "\n",
        "# Define the sequence lengths, max number of words and embedding dimensions\n",
        "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 300 \n",
        "\n",
        "# Top 20000 frequently occurring words\n",
        "MAX_NB_WORDS = 20000 \n",
        " \n",
        "# Get the frequently occurring words\n",
        " tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
        "tokenizer.fit_on_texts(train.Email) \n",
        "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
        "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
        "\n",
        "# dictionary containing words and their index\n",
        "word_index = tokenizer.word_index \n",
        "# print(tokenizer.word_index) \n",
        "# total words in the corpus\n",
        "print('Found %s unique tokens.' % len(word_index)) \n",
        "\n",
        "# get only the top frequent words on train\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
        "\n",
        "# get only the top frequent words on test\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsjzJcdZgEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train['Target']\n",
        "test_labels = test['Target']\n",
        "\n",
        "#import library\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# converts the character array to numeric array. Assigns levels to unique labels.\n",
        "\n",
        "le = LabelEncoder() \n",
        "le.fit(train_labels)\n",
        "train_labels = le.transform(train_labels)\n",
        "test_labels = le.transform(test_labels)\n",
        "\n",
        "print(le.classes_)\n",
        "print(np.unique(train_labels, return_counts=True))\n",
        "print(np.unique(test_labels, return_counts=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiyFS9bIZgEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changing data types\n",
        "labels_train = to_categorical(np.asarray(train_labels))\n",
        "labels_test = to_categorical(np.asarray(test_labels))\n",
        "print('Shape of data tensor:', train_data.shape)\n",
        "print('Shape of label tensor:', labels_train.shape)\n",
        "print('Shape of label tensor:', labels_test.shape)\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "print(MAX_SEQUENCE_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTYeMh5YZgEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries \n",
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Sequential\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySGGiAXwZgEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Training CNN 1D model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        " optimizer='rmsprop',\n",
        " metrics=['acc'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=64,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krTfnl2YZgEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictions on test data\n",
        "\n",
        "predicted=model.predict(test_data)\n",
        "predicted\n",
        "\n",
        "#model evaluation\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted.round()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4dq6-qBZgEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import library\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "\n",
        "#model training\n",
        "\n",
        "print('Training SIMPLERNN model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S3LbZz7ZgEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction on test data\n",
        "predicted_Srnn=model.predict(test_data)\n",
        "predicted_Srnn\n",
        "\n",
        "#model evaluation\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq7LgQE4ZgE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model training\n",
        "\n",
        "print('Training LSTM model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbbQ8-MZgE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction on text data\n",
        "predicted_lstm=model.predict(test_data)\n",
        "predicted_lstm\n",
        "\n",
        "#model evaluation \n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_lstm.round()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRIC_qb8ZgE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model training\n",
        "\n",
        "print('Training Bidirectional LSTM model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=3,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xk4LzSwZgE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction on test data\n",
        "\n",
        "predicted_blstm=model.predict(test_data)\n",
        "predicted_blstm\n",
        "\n",
        "#model evaluation\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_blstm.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_blstm.round()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMLWUHRYZgE-",
        "colab_type": "raw"
      },
      "source": [
        "Recipe 6-3. Next word/sequence of words suggestion – Next word prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5si5sBMZgE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Just selecting emails and connverting it into list\n",
        "Email_Data = file_content[[ 'v2']]\n",
        "\n",
        "list_data = Email_Data.values.tolist()\n",
        "list_data \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xWrf2bZgFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import scipy \n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "tokenizer = ToktokTokenizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI1pEmyVZgFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting list to string\n",
        "from collections import Iterable\n",
        "\n",
        "\n",
        "def flatten(items):\n",
        "    \"\"\"Yield items from any nested iterable\"\"\"\n",
        "    for x in items:\n",
        "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
        "            for sub_x in flatten(x):\n",
        "                yield sub_x\n",
        "        else:\n",
        "            yield x\n",
        "\n",
        "\n",
        "TextData=list(flatten(list_data))  \n",
        "TextData = ''.join(TextData) \n",
        "\n",
        "# Remove unwanted lines and converting into lower case\n",
        "TextData = TextData.replace('\\n','')\n",
        "TextData = TextData.lower() \n",
        "\n",
        "pattern = r'[^a-zA-z0-9\\s]' \n",
        "TextData = re.sub(pattern, '', ''.join(TextData)) \n",
        "\n",
        "# Tokenizing\n",
        "\n",
        "tokens = tokenizer.tokenize(TextData)\n",
        "tokens = [token.strip() for token in tokens] \n",
        "\n",
        "# get the distinct words and sort it\n",
        "\n",
        "word_counts = collections.Counter(tokens)\n",
        "word_c = len(word_counts)\n",
        "print(word_c)\n",
        "\n",
        "distinct_words = [x[0] for x in word_counts.most_common()]\n",
        "distinct_words_sorted = list(sorted(distinct_words)) \n",
        "\n",
        "\n",
        "# Generate indexing for all words\n",
        "\n",
        "word_index = {x: i for i, x in enumerate(distinct_words_sorted)} \n",
        "\n",
        "\n",
        "# decide on sentence lenght\n",
        "\n",
        "sentence_length = 25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwe2-XLKZgFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "# Generate the data for the model\n",
        "\n",
        "#input = the input sentence to the model with index \n",
        "#output = output of the model with index\n",
        "\n",
        "InputData = []\n",
        "OutputData = []\n",
        "\n",
        "for i in range(0, word_c - sentence_length, 1):\n",
        "    X = tokens[i:i + sentence_length]\n",
        "    Y = tokens[i + sentence_length]\n",
        "    InputData.append([word_index[char] for char in X])\n",
        "    OutputData.append(word_index[Y])\n",
        "\n",
        "print (InputData[:1])\n",
        "print (\"\\n\")\n",
        "print(OutputData[:1]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYldKpSTZgFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate  X \n",
        "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
        "\n",
        "\n",
        "# One hot encode the output variable\n",
        "Y = np_utils.to_categorical(OutputData) \n",
        "\n",
        "Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGGuEMkcZgFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        " \n",
        "#define the checkpoint\n",
        "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_name_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks = [checkpoint] \n",
        "\n",
        "#fit the model\n",
        "model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vca0yNZHZgFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "file_name = \"weights-improvement-05-6.8213.hdf5\"\n",
        "model.load_weights(file_name)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-hl7j1ZgFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating random sequence\n",
        "start = numpy.random.randint(0, len(InputData))\n",
        "input_sent = InputData[start]\n",
        "\n",
        "# Generate index of the next word of the email \n",
        "\n",
        "X = numpy.reshape(input_sent, (1, len(input_sent), 1))\n",
        "predict_word = model.predict(X, verbose=0)\n",
        "index = numpy.argmax(predict_word)\n",
        "\n",
        "print(input_sent)\n",
        "print (\"\\n\")\n",
        "print(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZipAx-SZgFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert these indexes back to words\n",
        "\n",
        "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
        "result = word_index_rev[index]\n",
        "sent_in = [word_index_rev[value] for value in input_sent]\n",
        "\n",
        "print(sent_in)\n",
        "print (\"\\n\")\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ6ZWjioZgFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFk5NNa9ZgFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHavfFTcZgFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}