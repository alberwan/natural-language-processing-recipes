{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Chapter 6.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6oUZeKEZgEH",
        "colab_type": "text"
      },
      "source": [
        "# Recipe 6-1. Retrieving Information\n",
        "Information retrieval is one of the highly used applications of NLP and it is\n",
        "quite tricky. The meaning of the words or sentences not only depends on\n",
        "the exact words used but also on the context and meaning. Two sentences\n",
        "may be of completely different words but can convey the same meaning.\n",
        "We should be able to capture that as well.\n",
        "An information retrieval (IR) system allows users to efficiently\n",
        "search documents and retrieve meaningful information based on a\n",
        "search text/query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZCfS1CgabyC",
        "colab_type": "text"
      },
      "source": [
        "## Problem\n",
        "\n",
        "Information retrieval using word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kxrjCHgk-6r",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-1 Import the libraries\n",
        "Here are the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRW6zxGsZgEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "12ffa214-2fd5-45a0-ca58-ffc0ff53bd62"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np \n",
        "import nltk\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import scipy \n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')  \n",
        "print(stopword_list) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTno6UalI_r",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-2 Create/import documents\n",
        "Randomly taking sentences from the internet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phlhq5yKZgEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8d5c1cd1-9790-4575-e59e-50814780f19e"
      },
      "source": [
        "# Randomly taking sentences from internet \n",
        "\n",
        "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ] \n",
        "     \n",
        "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
        "\n",
        "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\n",
        "\n",
        "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]\n",
        "\n",
        "# Put all the documents in one list\n",
        "\n",
        "fin= Doc1+Doc2+Doc3+Doc4\n",
        "\n",
        "print(fin)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.', 'Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.', 'He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.', 'But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtXNOWr_lVTG",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-3 Download word2vec\n",
        "As mentioned earlier, we are going to use the word embeddings to solve\n",
        "this problem. Download word2vec from the below link:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWclK0vIle9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "4ce23033-295f-4205-f2fe-d8a20374ae55"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#!wget -P /root/input/ -c \"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "#load the model\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary = True);\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 19:25:41--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.32.198\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.32.198|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  35.0MB/s    in 46s     \n",
            "\n",
            "2020-08-26 19:26:28 (34.2 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHd1Vb-wO1N",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-4 Create IR [information retrieval] system\n",
        "Now we build the information retrieval system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCPVRIf3ZgEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing \n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' \n",
        "    text = re.sub(pattern, '', ''.join(text))\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
        "\n",
        "def get_embedding(word):\n",
        "    if word in model.wv.vocab:\n",
        "        #return model[x]\n",
        "        return model[word]\n",
        "    else:\n",
        "        return np.zeros(300)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOxWM3Iwq_r",
        "colab_type": "text"
      },
      "source": [
        "For every document, we will get a lot of vectors based on the number of\n",
        "words present. We need to calculate the average vector for the document\n",
        "through taking a mean of all the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY1g7SglZgER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b7480a93-bc96-4b46-f0ce-a27f075f2bb7"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# Getting average vector for each document \n",
        "out_dict =  {}\n",
        "for sen in fin:\n",
        "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
        "    dict = { sen : (average_vector) }\n",
        "    out_dict.update(dict)\n",
        "\n",
        "# Function to calculate the similarity between the query vector and document vector\n",
        "\n",
        "def get_sim(query_embedding, average_vector_doc):\n",
        "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
        "    return sim\n",
        "\n",
        "# Rank all the documents based on the similarity to get relevant docs\n",
        "\n",
        "def Ranked_documents(query):\n",
        "    query_words =  (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
        "    rank = []\n",
        "    for k,v in out_dict.items():\n",
        "        rank.append((k, get_sim(query_words, v)))\n",
        "    rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
        "    print('Ranked Documents :')\n",
        "    return rank\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Ck5wxFxQsS",
        "colab_type": "text"
      },
      "source": [
        "# Step 1-5 Results and applications\n",
        "Let’s see how the information retrieval system we built is working with a\n",
        "couple of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjMsJCqpZgEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6ab6964a-5e2e-4e87-ed79-af085bd86178"
      },
      "source": [
        "# Call the IR function with a query\n",
        "# If you see, doc4 (on top in result), this will be most relevant for the\n",
        "# query “cricket” even though the word “cricket” is not even mentioned once\n",
        "# with the similarity of 0.449.\n",
        "\n",
        "Ranked_documents(\"cricket\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranked Documents :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.44954328830341783]),\n",
              " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.23973446930269127]),\n",
              " ('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.1832371201201335]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.17995061678671642])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szKnBM3CZgEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8cb3ff5d-5401-4547-80d6-aaee38689a7c"
      },
      "source": [
        "#Let’s take one more example as may be driving.\n",
        "# Again, since driving is connected to transport and the Motor Vehicles\n",
        "# Act, it pulls out the most relevant documents on top. The first 2 documents\n",
        "# are relevant to the query. \n",
        "\n",
        "Ranked_documents(\"driving\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranked Documents :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.3594728772380067]),\n",
              " ('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.19042557661139026]),\n",
              " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.1706653724240128]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.08872308406410134])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_2qkoTKRj_S",
        "colab_type": "text"
      },
      "source": [
        "# Recipe 6-2. Classifying Text with Deep Learning\n",
        "\n",
        "# Problem\n",
        "We want to build a text classification model using CNN, RNN, and LSTM.\n",
        "\n",
        "# Step 2-1 Understanding/defining business problem\n",
        "Email classification (spam or ham). We need to classify spam or ham email\n",
        "based on email content.\n",
        "\n",
        "# Step 2-2 Load data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqvGS5xa6oQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "cd1d0434-98bb-4467-d9f6-999e0320adb8"
      },
      "source": [
        "# download spam.csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "# csv_url = \"https://github.com/alberwan/Test-Data/blob/master/spam.csv\"\n",
        "# csv_url = \"https://drive.google.com/drive/folders/1ZOPzP0Id8B8d3xP4BMgLGoYLlPBRRKEs/spam.csv\"\n",
        "# csv_url = \"https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv\"\n",
        "# csv_url = \"spam.csv\"\n",
        "\n",
        "# csv_url = \"https://www.kaggle.com/ishansoni/sms-spam-collection-dataset?select=spam.csv\"\n",
        "s = requests.get(csv_url).content\n",
        "# file_content = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "# file_content = pd.read_csv(csv_url, encoding=\"ISO-8859-1\")\n",
        "# file_content = pd.read_csv(csv_url)\n",
        "file_content = pd.read_csv('/content/sample_data/spam.csv')\n",
        "file_content.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH6iNKb-pwoO",
        "colab_type": "text"
      },
      "source": [
        "# Step 2-3 Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWxAmCbZgEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "f78b6b07-410f-4658-c066-c6254f00e1ba"
      },
      "source": [
        "#check sample content in the email\n",
        "file_content['text'][1]\n",
        "\n",
        "#Import library\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Remove stop words\n",
        "stop = stopwords.words('english')\n",
        "# file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "file_content['text'] = file_content['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "# Delete unwanted columns\n",
        "Email_Data = file_content[['label', 'text']]\n",
        "\n",
        "# Rename column names\n",
        "Email_Data = Email_Data.rename(columns={\"label\":\"Target\", \"text\":\"Email\"})\n",
        "Email_Data.head()\n",
        "\n",
        "#Delete punctuations, convert text in lower case and delete the double space \n",
        "\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub(' ', ' ', x))\n",
        "Email_Data['Email'].head(5)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    go jurong point crazy available bugis n great ...\n",
              "1                              ok lar joking wif u oni\n",
              "2    free entry 2 wkly comp win fa cup final tkts 2...\n",
              "3                  u dun say early hor u c already say\n",
              "4             nah i think goes usf lives around though\n",
              "Name: Email, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YObrUqR6ZgEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separating text(input) and target classes\n",
        "\n",
        "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
        "list_classes = [\"Target\"]\n",
        "target = Email_Data[list_classes].values\n",
        "\n",
        "\n",
        "To_Process=Email_Data[['Email', 'Target']]\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQj5O-dsqXRF",
        "colab_type": "text"
      },
      "source": [
        "# Step 2-4 Data preparation for model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiLUaKc1ZgEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f7e4e88a-a6d3-4735-c0d2-94d3a6f8c4dc"
      },
      "source": [
        "#Train and test split with 80:20 ratio\n",
        "train, test = train_test_split(To_Process, test_size=0.2) \n",
        "\n",
        "# Define the sequence lengths, max number of words and embedding dimensions\n",
        "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 300 \n",
        "\n",
        "# Top 20000 frequently occurring words\n",
        "MAX_NB_WORDS = 20000 \n",
        " \n",
        "# Get the frequently occurring words\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
        "tokenizer.fit_on_texts(train.Email) \n",
        "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
        "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
        "\n",
        "# dictionary containing words and their index\n",
        "word_index = tokenizer.word_index \n",
        "# print(tokenizer.word_index) \n",
        "# total words in the corpus\n",
        "print('Found %s unique tokens.' % len(word_index)) \n",
        "\n",
        "# get only the top frequent words on train\n",
        "import tensorflow as tf\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
        "\n",
        "# get only the top frequent words on test\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8451 unique tokens.\n",
            "(4457, 300)\n",
            "(1115, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsjzJcdZgEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4b362ddb-1df8-4b74-bd72-55eea8f8cb6a"
      },
      "source": [
        "train_labels = train['Target']\n",
        "test_labels = test['Target']\n",
        "\n",
        "#import library\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# converts the character array to numeric array. Assigns levels to unique labels.\n",
        "\n",
        "le = LabelEncoder() \n",
        "le.fit(train_labels)\n",
        "train_labels = le.transform(train_labels)\n",
        "test_labels = le.transform(test_labels)\n",
        "\n",
        "print(le.classes_)\n",
        "print(np.unique(train_labels, return_counts=True))\n",
        "print(np.unique(test_labels, return_counts=True))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham' 'spam']\n",
            "(array([0, 1]), array([3862,  595]))\n",
            "(array([0, 1]), array([963, 152]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiyFS9bIZgEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e3fe005b-0b26-4b3f-81ec-cf8fe94807c8"
      },
      "source": [
        "# changing data types\n",
        "from keras.utils import to_categorical\n",
        "labels_train = to_categorical(np.asarray(train_labels))\n",
        "labels_test = to_categorical(np.asarray(test_labels))\n",
        "print('Shape of data tensor:', train_data.shape)\n",
        "print('Shape of label tensor:', labels_train.shape)\n",
        "print('Shape of label tensor:', labels_test.shape)\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "print(MAX_SEQUENCE_LENGTH)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (4457, 300)\n",
            "Shape of label tensor: (4457, 2)\n",
            "Shape of label tensor: (1115, 2)\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmBRr-9gxci5",
        "colab_type": "text"
      },
      "source": [
        "# Step 2-5 Model building and predicting\n",
        "We are building the models using different deep learning approaches\n",
        "like CNN, RNN, LSTM, and Bidirectional LSTM and comparing the\n",
        "performance of each model using different accuracy metrics.\n",
        "\n",
        "We can now define our CNN model.\n",
        "\n",
        "Here we define a single hidden layer with 128 memory units. The\n",
        "network uses a dropout with a probability of 0.5. The output layer is a\n",
        "dense layer using the softmax activation function to output a probability\n",
        "prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTYeMh5YZgEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries \n",
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Sequential\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySGGiAXwZgEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "0dc3dcd7-dad3-45e3-8e16-93584782eba4"
      },
      "source": [
        "print('Training CNN 1D model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        " optimizer='rmsprop',\n",
        " metrics=['acc'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=64,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training CNN 1D model.\n",
            "Epoch 1/5\n",
            "70/70 [==============================] - 17s 240ms/step - loss: 0.4979 - acc: 0.7891 - val_loss: 0.8409 - val_acc: 0.8637\n",
            "Epoch 2/5\n",
            "70/70 [==============================] - 16s 234ms/step - loss: 0.1952 - acc: 0.9260 - val_loss: 1.1245 - val_acc: 0.8637\n",
            "Epoch 3/5\n",
            "70/70 [==============================] - 16s 235ms/step - loss: 0.0901 - acc: 0.9749 - val_loss: 0.3349 - val_acc: 0.8637\n",
            "Epoch 4/5\n",
            "70/70 [==============================] - 16s 234ms/step - loss: 0.0635 - acc: 0.9818 - val_loss: 0.3338 - val_acc: 0.8655\n",
            "Epoch 5/5\n",
            "70/70 [==============================] - 16s 234ms/step - loss: 0.0391 - acc: 0.9904 - val_loss: 0.2952 - val_acc: 0.8673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5b47ce0ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krTfnl2YZgEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "31e7fc7a-4792-40e7-eefe-d24c6d3623e7"
      },
      "source": [
        "#predictions on test data\n",
        "\n",
        "predicted=model.predict(test_data)\n",
        "predicted\n",
        "\n",
        "#model evaluation\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted.round()))\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: [0.86678668 1.        ]\n",
            "recall: [1.         0.02631579]\n",
            "fscore: [0.92864031 0.05128205]\n",
            "support: [963 152]\n",
            "############################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       963\n",
            "           1       1.00      0.03      0.05       152\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      1115\n",
            "   macro avg       0.93      0.51      0.49      1115\n",
            "weighted avg       0.88      0.87      0.81      1115\n",
            " samples avg       0.87      0.87      0.87      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4dq6-qBZgEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "69822f7f-bd97-41f4-a135-a8dfd0b88a51"
      },
      "source": [
        "#Now define RNN model\n",
        "#import library\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "\n",
        "#model training\n",
        "\n",
        "print('Training SIMPLERNN model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training SIMPLERNN model.\n",
            "Epoch 1/5\n",
            "279/279 [==============================] - 25s 89ms/step - loss: 0.4692 - accuracy: 0.8874 - val_loss: 0.3522 - val_accuracy: 0.9094\n",
            "Epoch 2/5\n",
            "279/279 [==============================] - 25s 89ms/step - loss: 0.2131 - accuracy: 0.9592 - val_loss: 0.2864 - val_accuracy: 0.9211\n",
            "Epoch 3/5\n",
            "279/279 [==============================] - 25s 89ms/step - loss: 0.1075 - accuracy: 0.9843 - val_loss: 0.2759 - val_accuracy: 0.9166\n",
            "Epoch 4/5\n",
            "279/279 [==============================] - 25s 89ms/step - loss: 0.0623 - accuracy: 0.9933 - val_loss: 0.2874 - val_accuracy: 0.9076\n",
            "Epoch 5/5\n",
            "279/279 [==============================] - 25s 89ms/step - loss: 0.0387 - accuracy: 0.9960 - val_loss: 0.2947 - val_accuracy: 0.9076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5bb04a7518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0C8rw274F7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "8dac8251-8ef1-4192-f82d-4d6a03926eae"
      },
      "source": [
        "# prediction on test data\n",
        "predicted_Srnn=model.predict(test_data)\n",
        "predicted_Srnn"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98101735, 0.01898264],\n",
              "       [0.9854204 , 0.01457962],\n",
              "       [0.64395374, 0.35604626],\n",
              "       ...,\n",
              "       [0.99076146, 0.00923848],\n",
              "       [0.639665  , 0.36033493],\n",
              "       [0.95951307, 0.04048699]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S3LbZz7ZgEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "2f1ba6ee-e843-4463-a333-b174e2016dc5"
      },
      "source": [
        "#model evaluation\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: [0.93522267 0.69291339]\n",
            "recall: [0.95950156 0.57894737]\n",
            "fscore: [0.94720656 0.63082437]\n",
            "support: [963 152]\n",
            "############################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       963\n",
            "           1       0.69      0.58      0.63       152\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      1115\n",
            "   macro avg       0.81      0.77      0.79      1115\n",
            "weighted avg       0.90      0.91      0.90      1115\n",
            " samples avg       0.91      0.91      0.91      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWHp2Goq4rtY",
        "colab_type": "text"
      },
      "source": [
        "# Below is LSTM (Long Short-Term Memory) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq7LgQE4ZgE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "802fc0d7-e3b7-43f4-ecd3-c8ce5f0fd96a"
      },
      "source": [
        "#model training\n",
        "\n",
        "print('Training LSTM model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "# model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
        "model.add(LSTM(16, activation='relu', recurrent_activation='hard_sigmoid',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten()) \n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=5,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training LSTM model.\n",
            "Epoch 1/5\n",
            "279/279 [==============================] - 63s 226ms/step - loss: 0.1311 - accuracy: 0.9545 - val_loss: 0.2826 - val_accuracy: 0.9749\n",
            "Epoch 2/5\n",
            "279/279 [==============================] - 65s 232ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.0745 - val_accuracy: 0.9857\n",
            "Epoch 3/5\n",
            "279/279 [==============================] - 63s 224ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0980 - val_accuracy: 0.9812\n",
            "Epoch 4/5\n",
            "279/279 [==============================] - 63s 224ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0910 - val_accuracy: 0.9830\n",
            "Epoch 5/5\n",
            "279/279 [==============================] - 64s 228ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0769 - val_accuracy: 0.9857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5baf4694e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7aSw_vz8Opg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "88e43fb5-f7e5-498a-f39d-d3330cb5eda5"
      },
      "source": [
        "#prediction on text data\n",
        "predicted_lstm=model.predict(test_data)\n",
        "predicted_lstm"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.3131465e-01, 6.8685375e-02],\n",
              "       [9.9458694e-01, 5.4129958e-03],\n",
              "       [9.8875856e-01, 1.1241476e-02],\n",
              "       ...,\n",
              "       [9.9999952e-01, 4.7185071e-07],\n",
              "       [9.9999356e-01, 6.4308206e-06],\n",
              "       [1.0000000e+00, 3.1326636e-08]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbbQ8-MZgE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "eae1ec69-b781-429b-9467-73330ff6ab03"
      },
      "source": [
        "#model evaluation \n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_lstm.round()))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: [0.98564103 0.98571429]\n",
            "recall: [0.99792316 0.90789474]\n",
            "fscore: [0.99174407 0.94520548]\n",
            "support: [963 152]\n",
            "############################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       963\n",
            "           1       0.99      0.91      0.95       152\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1115\n",
            "   macro avg       0.99      0.95      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            " samples avg       0.99      0.99      0.99      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYGCUlov8zEp",
        "colab_type": "text"
      },
      "source": [
        "# Finally, let’s see what is Bidirectional LSTM and implement the same.\n",
        "As we know, LSTM preserves information from inputs using the\n",
        "hidden state. In bidirectional LSTMs, inputs are fed in two ways: one\n",
        "from previous to future and the other going backward from future to\n",
        "past, helping in learning future representation as well. Bidirectional\n",
        "LSTMs are known for producing very good results as they are capable of\n",
        "understanding the context better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRIC_qb8ZgE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "dc61814a-faff-40ce-c25e-092f92d565c3"
      },
      "source": [
        "#model training\n",
        "\n",
        "print('Training Bidirectional LSTM model.')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS,\n",
        " EMBEDDING_DIM,\n",
        " input_length=MAX_SEQUENCE_LENGTH\n",
        " ))\n",
        "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_data, labels_train,\n",
        " batch_size=16,\n",
        " epochs=3,\n",
        " validation_data=(test_data, labels_test))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Bidirectional LSTM model.\n",
            "Epoch 1/3\n",
            "279/279 [==============================] - 149s 535ms/step - loss: 0.1521 - accuracy: 0.9513 - val_loss: 0.0496 - val_accuracy: 0.9848\n",
            "Epoch 2/3\n",
            "279/279 [==============================] - 149s 533ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.0591 - val_accuracy: 0.9874\n",
            "Epoch 3/3\n",
            "279/279 [==============================] - 149s 533ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0710 - val_accuracy: 0.9821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5ba1d956a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q62jjkro-1vE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c3e44d66-67ee-4a13-b083-767dfaec5e2e"
      },
      "source": [
        "# prediction on test data\n",
        "\n",
        "predicted_blstm=model.predict(test_data)\n",
        "predicted_blstm"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.8216712e-01, 1.7832812e-02],\n",
              "       [9.9993086e-01, 6.9191941e-05],\n",
              "       [9.9416107e-01, 5.8389390e-03],\n",
              "       ...,\n",
              "       [9.9994600e-01, 5.3950647e-05],\n",
              "       [9.9996638e-01, 3.3666583e-05],\n",
              "       [9.9999869e-01, 1.3665087e-06]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xk4LzSwZgE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "bc1d2519-1728-40f3-ece6-5696e67e51d1"
      },
      "source": [
        "#model evaluation\n",
        "#We will see that Bidirectional LSTM outperforms the rest of the algorithms.\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted_blstm.round())\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n",
        "print(\"############################\")\n",
        "\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted_blstm.round()))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: [0.9825998  0.97826087]\n",
            "recall: [0.99688474 0.88815789]\n",
            "fscore: [0.98969072 0.93103448]\n",
            "support: [963 152]\n",
            "############################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       963\n",
            "           1       0.98      0.89      0.93       152\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      1115\n",
            "   macro avg       0.98      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            " samples avg       0.98      0.98      0.98      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMLWUHRYZgE-",
        "colab_type": "raw"
      },
      "source": [
        "Recipe 6-3. Next word/sequence of words suggestion – Next word prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5si5sBMZgE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Just selecting emails and connverting it into list\n",
        "Email_Data = file_content[[ 'v2']]\n",
        "\n",
        "list_data = Email_Data.values.tolist()\n",
        "list_data \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xWrf2bZgFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import scipy \n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "tokenizer = ToktokTokenizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI1pEmyVZgFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting list to string\n",
        "from collections import Iterable\n",
        "\n",
        "\n",
        "def flatten(items):\n",
        "    \"\"\"Yield items from any nested iterable\"\"\"\n",
        "    for x in items:\n",
        "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
        "            for sub_x in flatten(x):\n",
        "                yield sub_x\n",
        "        else:\n",
        "            yield x\n",
        "\n",
        "\n",
        "TextData=list(flatten(list_data))  \n",
        "TextData = ''.join(TextData) \n",
        "\n",
        "# Remove unwanted lines and converting into lower case\n",
        "TextData = TextData.replace('\\n','')\n",
        "TextData = TextData.lower() \n",
        "\n",
        "pattern = r'[^a-zA-z0-9\\s]' \n",
        "TextData = re.sub(pattern, '', ''.join(TextData)) \n",
        "\n",
        "# Tokenizing\n",
        "\n",
        "tokens = tokenizer.tokenize(TextData)\n",
        "tokens = [token.strip() for token in tokens] \n",
        "\n",
        "# get the distinct words and sort it\n",
        "\n",
        "word_counts = collections.Counter(tokens)\n",
        "word_c = len(word_counts)\n",
        "print(word_c)\n",
        "\n",
        "distinct_words = [x[0] for x in word_counts.most_common()]\n",
        "distinct_words_sorted = list(sorted(distinct_words)) \n",
        "\n",
        "\n",
        "# Generate indexing for all words\n",
        "\n",
        "word_index = {x: i for i, x in enumerate(distinct_words_sorted)} \n",
        "\n",
        "\n",
        "# decide on sentence lenght\n",
        "\n",
        "sentence_length = 25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwe2-XLKZgFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "# Generate the data for the model\n",
        "\n",
        "#input = the input sentence to the model with index \n",
        "#output = output of the model with index\n",
        "\n",
        "InputData = []\n",
        "OutputData = []\n",
        "\n",
        "for i in range(0, word_c - sentence_length, 1):\n",
        "    X = tokens[i:i + sentence_length]\n",
        "    Y = tokens[i + sentence_length]\n",
        "    InputData.append([word_index[char] for char in X])\n",
        "    OutputData.append(word_index[Y])\n",
        "\n",
        "print (InputData[:1])\n",
        "print (\"\\n\")\n",
        "print(OutputData[:1]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYldKpSTZgFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate  X \n",
        "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
        "\n",
        "\n",
        "# One hot encode the output variable\n",
        "Y = np_utils.to_categorical(OutputData) \n",
        "\n",
        "Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGGuEMkcZgFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        " \n",
        "#define the checkpoint\n",
        "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_name_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks = [checkpoint] \n",
        "\n",
        "#fit the model\n",
        "model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vca0yNZHZgFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "file_name = \"weights-improvement-05-6.8213.hdf5\"\n",
        "model.load_weights(file_name)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-hl7j1ZgFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating random sequence\n",
        "start = numpy.random.randint(0, len(InputData))\n",
        "input_sent = InputData[start]\n",
        "\n",
        "# Generate index of the next word of the email \n",
        "\n",
        "X = numpy.reshape(input_sent, (1, len(input_sent), 1))\n",
        "predict_word = model.predict(X, verbose=0)\n",
        "index = numpy.argmax(predict_word)\n",
        "\n",
        "print(input_sent)\n",
        "print (\"\\n\")\n",
        "print(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZipAx-SZgFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert these indexes back to words\n",
        "\n",
        "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
        "result = word_index_rev[index]\n",
        "sent_in = [word_index_rev[value] for value in input_sent]\n",
        "\n",
        "print(sent_in)\n",
        "print (\"\\n\")\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ6ZWjioZgFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFk5NNa9ZgFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHavfFTcZgFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}